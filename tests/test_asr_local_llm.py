# coding=utf-8
# ASR + æœ¬åœ°LLM æ•´åˆæµ‹è¯•

import torch
import time
import os
from src.asr.asr_model import ASRModule
from transformers import AutoTokenizer, Qwen2VLForConditionalGeneration, BitsAndBytesConfig

def get_available_device():
    """
    è·å–å¯ç”¨çš„è®¾å¤‡
    """
    if torch.cuda.is_available():
        try:
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"GPU å¯ç”¨ï¼Œå†…å­˜: {gpu_mem:.2f}GB")
            return "cuda:0"
        except Exception as e:
            print(f"GPU æ£€æŸ¥å¤±è´¥: {e}")
            return "cpu"
    else:
        print("GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        return "cpu"

def load_local_llm(model_path):
    """
    åŠ è½½æœ¬åœ°LLMæ¨¡å‹
    """
    print(f"\nå¼€å§‹åŠ è½½æœ¬åœ°LLMæ¨¡å‹: {model_path}")
    start_time = time.time()
    
    device = get_available_device()
    
    try:
        if device == "cuda:0":
            try:
                print("å°è¯•ä½¿ç”¨ INT8 é‡åŒ–åŠ è½½æ¨¡å‹...")
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_threshold=6.0
                )
                
                model = Qwen2VLForConditionalGeneration.from_pretrained(
                    model_path,
                    quantization_config=quantization_config,
                    device_map=device,
                    trust_remote_code=True
                )
                print("æ¨¡å‹åŠ è½½æˆåŠŸ (INT8 é‡åŒ–)")
            except Exception as e:
                print(f"INT8 é‡åŒ–åŠ è½½å¤±è´¥: {e}")
                print("å°è¯•ä½¿ç”¨ BF16 ç²¾åº¦åŠ è½½æ¨¡å‹...")
                model = Qwen2VLForConditionalGeneration.from_pretrained(
                    model_path,
                    torch_dtype=torch.bfloat16,
                    device_map=device,
                    trust_remote_code=True
                )
                print("æ¨¡å‹åŠ è½½æˆåŠŸ (BF16 ç²¾åº¦)")
        else:
            print("ä½¿ç”¨ CPU æ¨¡å¼åŠ è½½æ¨¡å‹...")
            model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                device_map=device,
                trust_remote_code=True
            )
            print("æ¨¡å‹åŠ è½½æˆåŠŸ (CPU æ¨¡å¼)")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print("åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        end_time = time.time()
        print(f"LLMæ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        
        return model, tokenizer, device
        
    except Exception as e:
        print(f"LLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def generate_response(model, tokenizer, device, text, max_new_tokens=100):
    """
    ä½¿ç”¨æœ¬åœ°LLMç”Ÿæˆå“åº”
    """
    try:
        print(f"\nLLMå¤„ç†ä¸­...")
        start_time = time.time()
        
        # å°è¯•ä½¿ç”¨chatæ¥å£
        if hasattr(model, "chat"):
            print("ä½¿ç”¨chatæ¥å£...")
            response = model.chat(
                tokenizer,
                text,
                history=[],
                max_new_tokens=max_new_tokens
            )
        else:
            # ä½¿ç”¨generateæ–¹æ³•
            print("ä½¿ç”¨generateæ–¹æ³•...")
            
            # æ”¹è¿›promptæ ¼å¼ï¼Œæ·»åŠ ç®€æ´å›ç­”çš„æŒ‡ç¤º
            prompt = f"{text}\n\nè¯·ç”¨ç®€çŸ­çš„ä¸€å¥è¯å›ç­”ï¼Œä¸è¶…è¿‡50å­—ã€‚"
            
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            
            # è®¾ç½®åœæ­¢token
            if tokenizer.eos_token_id is not None:
                eos_token_id = tokenizer.eos_token_id
            else:
                eos_token_id = tokenizer.pad_token_id
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.2,
                    eos_token_id=eos_token_id,
                    pad_token_id=eos_token_id
                )
            
            # åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            input_length = inputs['input_ids'].shape[1]
            new_tokens = outputs[0][input_length:]
            response = tokenizer.decode(new_tokens, skip_special_tokens=True)
        
        end_time = time.time()
        print(f"LLMç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
        
        return response
    except Exception as e:
        print(f"LLMç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return "æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”å¤±è´¥ã€‚"

def test_asr_llm():
    """
    æµ‹è¯•ASR + æœ¬åœ°LLMæ•´åˆ
    """
    print("=== ASR + æœ¬åœ°LLM æ•´åˆæµ‹è¯• ===")
    
    # 1. åˆå§‹åŒ–ASR
    print("\n1. åˆå§‹åŒ–ASRæ¨¡å—...")
    try:
        asr = ASRModule(model_dir="./SenseVoice")
        print("âœ… ASRæ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ASRæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # 2. åŠ è½½æœ¬åœ°LLM
    print("\n2. åŠ è½½æœ¬åœ°LLMæ¨¡å‹...")
    model_path = "./models/qwen_vl/qwen/Qwen2-VL-2B"
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    model, tokenizer, device = load_local_llm(model_path)
    
    if not model or not tokenizer:
        print("âŒ LLMæ¨¡å‹åŠ è½½å¤±è´¥")
        return
    
    print("âœ… LLMæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # 3. å¼€å§‹æµ‹è¯•
    print("\n3. å¼€å§‹è¯­éŸ³è¯†åˆ«å’ŒLLMå¤„ç†...")
    print("è¯·è¯´è¯ï¼Œç³»ç»Ÿå°†è¯†åˆ«æ‚¨çš„è¯­éŸ³å¹¶ä½¿ç”¨æœ¬åœ°LLMç”Ÿæˆå“åº”...")
    print("ï¼ˆå½•éŸ³å°†æŒç»­5ç§’ï¼‰")
    
    try:
        # ASRè¯­éŸ³è¯†åˆ«
        print("\n--- å¼€å§‹å½•éŸ³ ---")
        start_time = time.time()
        recognized_text = asr.record_and_recognize(duration=5, use_vad=False)
        end_time = time.time()
        
        print(f"\n--- ASRè¯†åˆ«å®Œæˆ ---")
        print(f"è¯†åˆ«è€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"è¯†åˆ«ç»“æœ: {recognized_text}")
        
        if not recognized_text:
            print("âŒ æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³")
            return
        
        # LLMç”Ÿæˆå“åº”
        print(f"\n--- LLMç”Ÿæˆå“åº” ---")
        response = generate_response(model, tokenizer, device, recognized_text)
        
        print(f"\n--- æœ€ç»ˆç»“æœ ---")
        print(f"ç”¨æˆ·è¾“å…¥: {recognized_text}")
        print(f"LLMå“åº”: {response}")
        print("\nâœ… æµ‹è¯•æˆåŠŸï¼")
        
    except KeyboardInterrupt:
        print("\nğŸ”„ ç”¨æˆ·ç»ˆæ­¢æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_asr_llm()